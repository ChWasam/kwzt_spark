apiVersion: batch/v1
kind: Job
metadata:
  name: pyspark-eda
  labels:
    app: pyspark-eda
    version: v1
spec:
  backoffLimit: 3
  ttlSecondsAfterFinished: 3600
  template:
    metadata:
      labels:
        app: pyspark-eda
    spec:
      restartPolicy: OnFailure
      containers:
      - name: pyspark
        image: jupyter/pyspark-notebook:spark-3.5.0
        command: ["/bin/bash", "-c"]
        env:
          - name: HOME
            value: /tmp
          - name: USER
            value: spark
          - name: SPARK_HOME
            value: /usr/local/spark
          - name: JAVA_HOME
            value: /usr/lib/jvm/java-17-openjdk-amd64
          - name: HADOOP_CONF_DIR
            value: /tmp/hadoop-conf
          - name: SPARK_LOCAL_DIRS
            value: /tmp
          - name: HADOOP_USER_NAME
            value: spark
          - name: SPARK_USER
            value: spark
          - name: MPLCONFIGDIR
            value: /tmp/matplotlib
          - name: XDG_CACHE_HOME
            value: /tmp/.cache
          - name: PYTHONPATH
            value: /usr/local/spark/python:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip

        args:
        - |
          set -euo pipefail

          # === 0. Export USER variable explicitly ===
          export USER=spark
          export HADOOP_USER_NAME=spark

          echo "[INFO] Initializing PySpark EDA Job..."
          echo "[INFO] Running as user: $(whoami 2>/dev/null || echo $USER)"

          # === 1. Create writable directories ===
          mkdir -p /tmp/hadoop-conf \
                   /tmp/matplotlib \
                   /tmp/spark-warehouse \
                   /tmp/spark-events \
                   /tmp/.ivy2/{cache,local} \
                   /tmp/.m2/repository \
                   /tmp/.cache/pip

          chmod -R 777 /tmp/.ivy2 /tmp/.m2 /tmp/.cache

          # === 2. Minimal Hadoop config: disable Kerberos ===
          cat > $HADOOP_CONF_DIR/core-site.xml << 'EOF'
          <?xml version="1.0"?>
          <configuration>
            <property>
              <name>hadoop.security.authentication</name>
              <value>simple</value>
            </property>
            <property>
              <name>hadoop.security.authorization</name>
              <value>false</value>
            </property>
            <property>
              <name>hadoop.security.credential.provider.path</name>
              <value></value>
            </property>
            <property>
              <name>fs.file.impl</name>
              <value>org.apache.hadoop.fs.RawLocalFileSystem</value>
            </property>
          </configuration>
          EOF

          # === 3. ivysettings.xml: block remote downloads ===
          cat > /tmp/.ivy2/ivysettings.xml << 'EOF'
          <ivysettings>
            <settings defaultResolver="local-only"/>
            <resolvers>
              <chain name="local-only">
                <filesystem name="local" m2compatible="true">
                  <artifact pattern="/tmp/.ivy2/local/[organisation]/[module]/[revision]/[artifact]-[revision](-[classifier]).[ext]"/>
                  <ivy pattern="/tmp/.ivy2/local/[organisation]/[module]/[revision]/ivy-[revision].xml"/>
                </filesystem>
                <filesystem name="cache" m2compatible="true" checkmodified="true">
                  <artifact pattern="/tmp/.ivy2/cache/[organisation]/[module]/[type]s/[artifact]-[revision](-[classifier]).[ext]"/>
                  <ivy pattern="/tmp/.ivy2/cache/[organisation]/[module]/ivys/ivy-[revision].xml"/>
                </filesystem>
              </chain>
            </resolvers>
          </ivysettings>
          EOF

          # === 4. Install Python packages ===
          pip install --cache-dir=$XDG_CACHE_HOME/pip --quiet --no-warn-script-location \
              matplotlib seaborn pandas

          # === 5. Verify input file ===
          if [ ! -f /data/received_transactions.csv ]; then
            echo "ERROR: /data/received_transactions.csv NOT FOUND!"
            exit 1
          fi
          echo "Input file found: $(ls -lh /data/received_transactions.csv)"

          # === 6. Run PySpark EDA ===
          python << 'PYSPARK_EDA'
          import os
          from pyspark.sql import SparkSession
          from pyspark.sql.functions import col, sum as spark_sum, count, avg
          import matplotlib
          matplotlib.use('Agg')
          import matplotlib.pyplot as plt

          print("\nVerifying ivysettings.xml...")
          os.system("cat /tmp/.ivy2/ivysettings.xml")

          spark = (SparkSession.builder
              .appName("Transaction EDA")
              .master("local[2]")
              .config("spark.driver.host", "localhost")
              .config("spark.driver.bindAddress", "0.0.0.0")
              .config("spark.ui.enabled", "false")
              .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse")
              .config("spark.local.dir", "/tmp")
              # Bypass Kerberos + use raw local FS
              .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.RawLocalFileSystem")
              .config("spark.hadoop.hadoop.security.authentication", "simple")
              .config("spark.hadoop.hadoop.security.authorization", "false")
              .config("spark.authenticate", "false")
              .config("spark.jars.ivySettings", "/tmp/.ivy2/ivysettings.xml")
              .getOrCreate())

          spark.sparkContext.setLogLevel("ERROR")

          print("\nLoading data from /data/received_transactions.csv...")
          df = spark.read.csv("/data/received_transactions.csv", header=True, inferSchema=True)
          total_records = df.count()
          print(f"Data loaded: {total_records:,} records")

          # === Analysis 1: Sales by Category ===
          print("\nSales by Product Category:")
          category_sales = (df.groupBy("product_category")
              .agg(
                  count("transaction_id").alias("num_transactions"),
                  spark_sum("total_amount").alias("total_revenue")
              )
              .orderBy(col("total_revenue").desc()))
          category_pd = category_sales.toPandas()
          print(category_pd.to_string(index=False))

          # === Analysis 2: Payment Methods ===
          print("\nPayment Method Distribution:")
          payment_analysis = (df.groupBy("payment_method")
              .agg(count("transaction_id").alias("num_transactions"))
              .orderBy(col("num_transactions").desc()))
          payment_pd = payment_analysis.toPandas()
          print(payment_pd.to_string(index=False))

          # === Analysis 3: Top 10 Cities ===
          print("\nTop 10 Cities by Revenue:")
          city_revenue = (df.groupBy("city")
              .agg(
                  count("transaction_id").alias("num_transactions"),
                  spark_sum("total_amount").alias("total_revenue")
              )
              .orderBy(col("total_revenue").desc())
              .limit(10))
          city_pd = city_revenue.toPandas()
          print(city_pd.to_string(index=False))

          # === Generate Plots ===
          print("\nGenerating visualizations...")

          plt.figure(figsize=(12, 6))
          cat_sorted = category_pd.sort_values('total_revenue', ascending=False)
          plt.bar(cat_sorted['product_category'], cat_sorted['total_revenue'], color='teal')
          plt.xlabel('Product Category')
          plt.ylabel('Total Revenue ($)')
          plt.title('Total Revenue by Product Category')
          plt.xticks(rotation=45, ha='right')
          plt.tight_layout()
          plt.savefig('/data/plot_revenue_by_category.png', dpi=150, bbox_inches='tight')
          plt.close()
          print("Saved: /data/plot_revenue_by_category.png")

          plt.figure(figsize=(10, 8))
          plt.pie(payment_pd['num_transactions'],
                  labels=payment_pd['payment_method'],
                  autopct='%1.1f%%', startangle=90)
          plt.title('Payment Method Distribution')
          plt.tight_layout()
          plt.savefig('/data/plot_payment_methods.png', dpi=150)
          plt.close()
          print("Saved: /data/plot_payment_methods.png")

          plt.figure(figsize=(12, 8))
          plt.barh(city_pd['city'], city_pd['total_revenue'], color='coral')
          plt.xlabel('Total Revenue ($)')
          plt.ylabel('City')
          plt.title('Top 10 Cities by Revenue')
          plt.tight_layout()
          plt.savefig('/data/plot_top_cities.png', dpi=150, bbox_inches='tight')
          plt.close()
          print("Saved: /data/plot_top_cities.png")

          # === Summary ===
          total_revenue = df.agg(spark_sum("total_amount")).collect()[0][0] or 0.0
          avg_transaction = df.agg(avg("total_amount")).collect()[0][0] or 0.0

          print("\n" + "="*80)
          print(" ANALYSIS SUMMARY")
          print("="*80)
          print(f" Total Transactions : {total_records:,}")
          print(f" Total Revenue      : ${total_revenue:,.2f}")
          print(f" Average Transaction: ${avg_transaction:,.2f}")
          print("="*80)
          print("EDA Complete! All outputs saved to /data/")

          spark.stop()
          PYSPARK_EDA

        volumeMounts:
        - name: data
          mountPath: /data

        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 1500m
            memory: 4Gi

      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: transaction-data-pvc

      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault