{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-commerce Transaction Data Analysis with PySpark\n",
    "\n",
    "## DS551 - Kafka + Spark Assignment\n",
    "\n",
    "### Objective\n",
    "Perform Exploratory Data Analysis (EDA) on transaction data streamed through Kafka using PySpark.\n",
    "\n",
    "### Why PySpark instead of Pandas?\n",
    "- **Pandas**: Loads entire dataset into memory ‚Üí struggles with large files (1M+ rows)\n",
    "- **PySpark**: Distributed processing ‚Üí handles massive datasets efficiently\n",
    "- **Lazy Evaluation**: PySpark optimizes operations before execution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PySpark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    hour, dayofweek, to_timestamp, desc\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Spark Session\n",
    "\n",
    "**SparkSession**: Entry point to PySpark functionality. Think of it as the \"connection\" to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Transaction Data EDA\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check Spark version\n",
    "print(f\"‚úÖ Spark Session created!\")\n",
    "print(f\"üìä Spark Version: {spark.version}\")\n",
    "print(f\"üîó Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data from CSV\n",
    "\n",
    "Loading data that came through Kafka and was saved by the consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into Spark DataFrame\n",
    "df = spark.read.csv(\n",
    "    \"/home/jovyan/data/transactions.csv\",  # Path inside container\n",
    "    header=True,\n",
    "    inferSchema=True  # Automatically detect data types\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"üìä Total records: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n",
    "\n",
    "### 4.1 Schema (Column Types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "print(\"üìã Data Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 rows\n",
    "print(\"üìÑ Sample Data (first 10 rows):\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numeric columns\n",
    "print(\"üìà Statistical Summary:\")\n",
    "df.select(\"product_price\", \"quantity\", \"total_amount\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Check for Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count null values in each column\n",
    "from pyspark.sql.functions import col, sum as spark_sum, when, isnan, isnull\n",
    "\n",
    "null_counts = df.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "print(\"üîç Null Value Counts:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Analysis\n",
    "\n",
    "### 5.1 Sales by Product Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product category and calculate total sales\n",
    "category_sales = df.groupBy(\"product_category\") \\\n",
    "    .agg(\n",
    "        count(\"transaction_id\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\"))\n",
    "\n",
    "print(\"üí∞ Sales by Product Category:\")\n",
    "category_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sales by Payment Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze payment methods\n",
    "payment_analysis = df.groupBy(\"payment_method\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        avg(\"total_amount\").alias(\"avg_transaction\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"num_transactions\"))\n",
    "\n",
    "print(\"üí≥ Payment Method Analysis:\")\n",
    "payment_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Top 10 Cities by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top cities by total revenue\n",
    "city_revenue = df.groupBy(\"city\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        spark_sum(\"total_amount\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_revenue\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "print(\"üèôÔ∏è Top 10 Cities by Revenue:\")\n",
    "city_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Average Transaction Amount by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average transaction amount per category\n",
    "avg_transaction = df.groupBy(\"product_category\") \\\n",
    "    .agg(\n",
    "        avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "        spark_min(\"total_amount\").alias(\"min_amount\"),\n",
    "        spark_max(\"total_amount\").alias(\"max_amount\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_amount\"))\n",
    "\n",
    "print(\"üìä Average Transaction by Category:\")\n",
    "avg_transaction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Visualizations with Matplotlib\n",
    "\n",
    "### 6.1 Total Revenue by Product Category (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas for plotting\n",
    "category_sales_pd = category_sales.toPandas()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(category_sales_pd['product_category'], \n",
    "        category_sales_pd['total_revenue'],\n",
    "        color='steelblue')\n",
    "plt.xlabel('Product Category', fontsize=12)\n",
    "plt.ylabel('Total Revenue ($)', fontsize=12)\n",
    "plt.title('Total Revenue by Product Category', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Bar chart created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Transaction Amount Distribution (Histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for histogram (avoid memory issues)\n",
    "sample_amounts = df.select(\"total_amount\").sample(False, 0.1).toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(sample_amounts['total_amount'], bins=50, color='coral', edgecolor='black')\n",
    "plt.xlabel('Transaction Amount ($)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Transaction Amounts', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Histogram created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Payment Method Distribution (Pie Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert payment method data to Pandas\n",
    "payment_pd = payment_analysis.toPandas()\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\n",
    "plt.pie(payment_pd['num_transactions'], \n",
    "        labels=payment_pd['payment_method'],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=90)\n",
    "plt.title('Payment Method Distribution', fontsize=14, fontweight='bold')\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Pie chart created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Top 10 Cities by Revenue (Horizontal Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert city revenue data to Pandas\n",
    "city_revenue_pd = city_revenue.toPandas()\n",
    "\n",
    "# Create horizontal bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(city_revenue_pd['city'], \n",
    "         city_revenue_pd['total_revenue'],\n",
    "         color='mediumseagreen')\n",
    "plt.xlabel('Total Revenue ($)', fontsize=12)\n",
    "plt.ylabel('City', fontsize=12)\n",
    "plt.title('Top 10 Cities by Revenue', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Horizontal bar chart created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Number of Transactions by Category (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transaction count bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(category_sales_pd['product_category'], \n",
    "        category_sales_pd['num_transactions'],\n",
    "        color='mediumpurple')\n",
    "plt.xlabel('Product Category', fontsize=12)\n",
    "plt.ylabel('Number of Transactions', fontsize=12)\n",
    "plt.title('Number of Transactions by Product Category', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Transaction count chart created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "total_records = df.count()\n",
    "total_revenue = df.agg(spark_sum(\"total_amount\")).collect()[0][0]\n",
    "avg_transaction = df.agg(avg(\"total_amount\")).collect()[0][0]\n",
    "unique_customers = df.select(\"customer_id\").distinct().count()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"üìÑ Total Transactions: {total_records:,}\")\n",
    "print(f\"üí∞ Total Revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"üíµ Average Transaction: ${avg_transaction:,.2f}\")\n",
    "print(f\"üë• Unique Customers: {unique_customers:,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Top category\n",
    "top_category = category_sales.first()\n",
    "print(f\"\\nüèÜ Top Category: {top_category['product_category']}\")\n",
    "print(f\"   Revenue: ${top_category['total_revenue']:,.2f}\")\n",
    "print(f\"   Transactions: {top_category['num_transactions']:,}\")\n",
    "\n",
    "# Most popular payment method\n",
    "top_payment = payment_analysis.first()\n",
    "print(f\"\\nüí≥ Most Popular Payment Method: {top_payment['payment_method']}\")\n",
    "print(f\"   Transactions: {top_payment['num_transactions']:,}\")\n",
    "\n",
    "print(\"\\n‚úÖ EDA Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (optional - good practice)\n",
    "# spark.stop()\n",
    "print(\"‚úÖ Notebook execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Loading large datasets with PySpark\n",
    "2. ‚úÖ Performing aggregations and transformations\n",
    "3. ‚úÖ Creating visualizations with matplotlib\n",
    "4. ‚úÖ Analyzing e-commerce transaction patterns\n",
    "\n",
    "**Key Takeaways:**\n",
    "- PySpark handles large datasets efficiently through distributed processing\n",
    "- `.toPandas()` converts Spark DataFrames to Pandas for visualization\n",
    "- Lazy evaluation means operations are optimized before execution\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy to OpenShift/Kubernetes\n",
    "- Integrate with real-time Kafka streaming\n",
    "- Add more advanced analytics\n",
    "\n",
    "---\n",
    "**Author**: DS551 Student  \n",
    "**Date**: 2025-11-14  \n",
    "**Assignment**: Kafka + PySpark EDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
